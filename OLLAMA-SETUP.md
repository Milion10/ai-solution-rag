# Installation et Configuration d'Ollama

## ðŸ“¥ Installation

### Windows
1. TÃ©lÃ©chargez Ollama : https://ollama.ai/download/windows
2. ExÃ©cutez l'installeur `OllamaSetup.exe`
3. Ollama dÃ©marre automatiquement en arriÃ¨re-plan

### VÃ©rification
```powershell
ollama --version
```

## ðŸ¤– TÃ©lÃ©chargement du ModÃ¨le Mistral 7B

```powershell
ollama pull mistral:7b-instruct
```

**Taille du tÃ©lÃ©chargement :** ~4.1 GB (modÃ¨le Q4 quantifiÃ©)

## âœ… Test du ModÃ¨le

```powershell
ollama run mistral:7b-instruct
```

Dans la console interactive :
```
>>> Bonjour, peux-tu te prÃ©senter ?
>>> /bye
```

## ðŸ”Œ API Ollama

Ollama expose une API REST sur `http://localhost:11434`

### Test de santÃ©
```powershell
curl http://localhost:11434/api/tags
```

### Test de gÃ©nÃ©ration
```powershell
curl http://localhost:11434/api/generate -d '{
  "model": "mistral:7b-instruct",
  "prompt": "Dis bonjour en franÃ§ais",
  "stream": false
}'
```

## ðŸŽ¯ ModÃ¨les Alternatifs

Si Mistral est trop lourd pour votre machine :

```powershell
# Phi-3 Mini (3.8B paramÃ¨tres, ~2.3 GB)
ollama pull phi3:mini

# Llama 3.2 (3B paramÃ¨tres, ~2 GB)
ollama pull llama3.2:3b
```

Puis modifiez dans `ai/llm.py` :
```python
model: str = "phi3:mini"  # ou "llama3.2:3b"
```

## ðŸ“Š Utilisation MÃ©moire

- **Mistral 7B Q4** : ~4-5 GB RAM
- **Phi-3 Mini** : ~2-3 GB RAM
- **Llama 3.2 3B** : ~2-3 GB RAM

## ðŸ”§ Configuration AvancÃ©e

### Modifier le port Ollama (optionnel)
```powershell
# Variables d'environnement
$env:OLLAMA_HOST = "0.0.0.0:11434"
```

### Performance GPU (si disponible)
Ollama dÃ©tecte automatiquement votre GPU (NVIDIA/AMD) et l'utilise.

## ðŸš€ DÃ©marrage Automatique

Ollama dÃ©marre automatiquement avec Windows. Pour le gÃ©rer :

```powershell
# ArrÃªter Ollama
taskkill /IM ollama.exe /F

# RedÃ©marrer (via icÃ´ne systÃ¨me ou exÃ©cutez)
ollama serve
```

## ðŸ“š Documentation

- Site officiel : https://ollama.ai
- GitHub : https://github.com/ollama/ollama
- ModÃ¨les disponibles : https://ollama.ai/library
