# ğŸš€ Configuration Groq - Guide Rapide

## âœ… Configuration TerminÃ©e

Tu es maintenant configurÃ© avec **Groq** pour un dÃ©veloppement ultra-rapide !

### ğŸ¯ Avantages
- âš¡ **RÃ©ponses en <2 secondes** (au lieu de 40 secondes)
- ğŸ†“ **Gratuit** pour le dÃ©veloppement
- ğŸ”„ **Facile Ã  basculer** vers Ollama pour la production

---

## ğŸ“Š Comparaison Avant/AprÃ¨s

| CritÃ¨re | Avant (Ollama) | AprÃ¨s (Groq) |
|---------|----------------|--------------|
| **Temps de rÃ©ponse** | 40 secondes â±ï¸ | 1-2 secondes âš¡ |
| **ModÃ¨le** | Mistral 7B local | Llama 3.3 70B cloud |
| **CoÃ»t** | Gratuit | Gratuit (limites) |
| **Privacy** | 100% local ğŸ”’ | Cloud â˜ï¸ |

---

## ğŸ”§ Configuration Actuelle

### Fichier `.env` (Backend)
```env
LLM_PROVIDER=groq
GROQ_API_KEY=votre_clÃ©_api_groq_ici
GROQ_MODEL=llama-3.3-70b-versatile
```

### ModÃ¨les Groq Disponibles
- `llama-3.3-70b-versatile` (actuel) - Excellent pour tout usage
- `llama-3.1-70b-versatile` - Alternative stable
- `llama3-8b-8192` - Plus lÃ©ger, encore plus rapide
- `gemma2-9b-it` - Bon compromis vitesse/qualitÃ©

---

## ğŸ”„ Basculer entre Groq et Ollama

### Pour le dÃ©veloppement (Groq - rapide)
Dans `backend/.env` :
```env
LLM_PROVIDER=groq
```

### Pour la production (Ollama - local/privÃ©)
Dans `backend/.env` :
```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=mistral:7b-instruct
```

Puis redÃ©marrer le backend :
```bash
cd ai-solution
.\stop-app.ps1
.\start-app.ps1
```

---

## ğŸ§ª Tester la Configuration

### Test rapide
```bash
cd ai-solution/backend
..\..\.venv\Scripts\python.exe test_groq.py
```

### Test dans l'application
1. Ouvre http://localhost:3000/chat
2. Pose une question
3. La rÃ©ponse devrait arriver en **1-2 secondes** âš¡

---

## ğŸ“¦ Fichiers ModifiÃ©s

### Nouveaux fichiers
- âœ… `backend/ai/llm_factory.py` - Factory pour gÃ©rer les providers
- âœ… `backend/test_groq.py` - Script de test
- âœ… `backend/.env.example` - Template de configuration

### Fichiers modifiÃ©s
- âœ… `backend/.env` - Configuration Groq
- âœ… `backend/requirements.txt` - Ajout du package groq
- âœ… `backend/api/chat.py` - Import du nouveau factory

---

## ğŸ” SÃ©curitÃ©

âš ï¸ **Important** : Ta clÃ© API Groq est dans `.env` (ignorÃ© par Git).

### Renouveler la clÃ© API (si besoin)
1. Va sur https://console.groq.com/keys
2. RÃ©voque l'ancienne clÃ©
3. CrÃ©e une nouvelle clÃ©
4. Remplace dans `backend/.env`

---

## ğŸ“ˆ Limites Groq (Plan Gratuit)

- **RequÃªtes/minute** : ~30 requÃªtes
- **Tokens/minute** : ~6000 tokens
- **Tokens/jour** : IllimitÃ©

Pour le dÃ©veloppement, c'est **largement suffisant** ! ğŸ‰

---

## ğŸ†˜ Troubleshooting

### Erreur "Model decommissioned"
Le modÃ¨le n'est plus disponible. Change dans `.env` :
```env
GROQ_MODEL=llama-3.3-70b-versatile
```

### Erreur "API Key invalid"
VÃ©rifie que ta clÃ© est bien copiÃ©e dans `.env`

### RÃ©ponses lentes
Tu utilises peut-Ãªtre encore Ollama. VÃ©rifie :
```env
LLM_PROVIDER=groq  # Doit Ãªtre "groq" et pas "ollama"
```

---

## ğŸ“ Prochaines Ã‰tapes

1. âœ… **Teste l'application** - Les rÃ©ponses devraient Ãªtre ultra-rapides
2. ğŸ“ **DÃ©veloppe tranquillement** - Plus besoin d'attendre 40 secondes !
3. ğŸš€ **En production** - Bascule vers Ollama pour privacy totale

---

## ğŸ’¡ Astuce Pro

Pour basculer rapidement entre providers, crÃ©e des alias :
```powershell
# Dans ton profil PowerShell
function Use-Groq { (Get-Content backend\.env) -replace 'LLM_PROVIDER=ollama', 'LLM_PROVIDER=groq' | Set-Content backend\.env }
function Use-Ollama { (Get-Content backend\.env) -replace 'LLM_PROVIDER=groq', 'LLM_PROVIDER=ollama' | Set-Content backend\.env }
```

Ensuite :
```powershell
Use-Groq    # Passe Ã  Groq (dev rapide)
Use-Ollama  # Passe Ã  Ollama (prod locale)
```

---

**Tu es prÃªt ! Enjoy la vitesse de dÃ©veloppement ! ğŸš€**
