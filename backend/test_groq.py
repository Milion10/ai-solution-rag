"""
Script de test pour v√©rifier la configuration Groq
"""
import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from ai.llm_factory import get_llm_generator
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_groq_setup():
    """Teste la configuration et la connexion Groq"""
    print("\n" + "="*60)
    print("üß™ TEST CONFIGURATION GROQ")
    print("="*60)
    
    try:
        # 1. R√©cup√©rer le provider
        print("\n1Ô∏è‚É£ Initialisation du provider LLM...")
        llm = get_llm_generator()
        print(f"‚úÖ Provider initialis√©: {llm.__class__.__name__}")
        
        # 2. V√©rifier la sant√©
        print("\n2Ô∏è‚É£ V√©rification de la connexion...")
        if llm.check_health():
            print("‚úÖ Groq API accessible")
        else:
            print("‚ùå Groq API non accessible")
            return False
        
        # 3. Test de g√©n√©ration simple
        print("\n3Ô∏è‚É£ Test de g√©n√©ration simple...")
        prompt = "Explique en une phrase ce qu'est le RAG."
        print(f"üìù Prompt: {prompt}")
        
        response = llm.generate(prompt=prompt, max_tokens=100)
        print(f"‚úÖ R√©ponse re√ßue ({len(response)} caract√®res):")
        print(f"   {response[:200]}...")
        
        # 4. Test RAG (simul√©)
        print("\n4Ô∏è‚É£ Test de g√©n√©ration RAG...")
        mock_chunks = [
            {
                "filename": "doc_test.txt",
                "content": "Le RAG (Retrieval-Augmented Generation) est une technique qui combine la recherche d'information et la g√©n√©ration de texte.",
                "chunk_index": 0,
                "similarity": 0.85
            }
        ]
        
        rag_response = llm.generate_rag_response(
            query="Qu'est-ce que le RAG ?",
            context_chunks=mock_chunks
        )
        
        print(f"‚úÖ R√©ponse RAG g√©n√©r√©e:")
        print(f"   Answer: {rag_response['answer'][:200]}...")
        print(f"   Confidence: {rag_response['confidence']}%")
        print(f"   Sources: {len(rag_response['sources'])}")
        
        # 5. Test r√©ponse g√©n√©rale
        print("\n5Ô∏è‚É£ Test de r√©ponse g√©n√©rale...")
        gen_response = llm.generate_general_response(
            query="Quelle est la capitale de la France ?"
        )
        
        print(f"‚úÖ R√©ponse g√©n√©rale:")
        print(f"   {gen_response['answer'][:200]}...")
        
        print("\n" + "="*60)
        print("üéâ TOUS LES TESTS R√âUSSIS !")
        print("="*60)
        print("\nüí° Groq est op√©rationnel. Le backend devrait maintenant r√©pondre en <2 secondes.")
        print("   Pour retourner √† Ollama en production, changez LLM_PROVIDER=ollama dans .env\n")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = test_groq_setup()
    sys.exit(0 if success else 1)
