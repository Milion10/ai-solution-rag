"""
Service de gÃ©nÃ©ration de texte avec LLM local via Ollama
Alternative simple Ã  llama-cpp-python pour Windows
"""
import httpx
import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)


class LLMGenerator:
    """
    GÃ©nÃ¨re des rÃ©ponses avec un LLM local via Ollama API
    """
    
    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ):
        """
        Initialise le gÃ©nÃ©rateur LLM
        
        Args:
            base_url: URL de l'API Ollama
            model: Nom du modÃ¨le (auto-dÃ©tectÃ© si None)
            temperature: ContrÃ´le la crÃ©ativitÃ© (0.0-1.0)
            max_tokens: Nombre maximum de tokens gÃ©nÃ©rÃ©s
        """
        self.base_url = base_url
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = httpx.Client(timeout=120.0)  # 2 min timeout pour gÃ©nÃ©ration
        
        # Auto-dÃ©tection du modÃ¨le si non spÃ©cifiÃ©
        if model is None:
            model = self._detect_model()
        
        self.model = model
        logger.info(f"LLMGenerator initialisÃ©: {model} @ {base_url}")
    
    
    def _detect_model(self) -> str:
        """DÃ©tecte automatiquement le premier modÃ¨le Mistral/Llama disponible"""
        try:
            response = self.client.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                if models:
                    # PrioritÃ©: mistral > llama > premier dispo
                    for model in models:
                        name = model["name"]
                        if "mistral" in name.lower():
                            logger.info(f"âœ… ModÃ¨le Mistral dÃ©tectÃ©: {name}")
                            return name
                    
                    for model in models:
                        name = model["name"]
                        if "llama" in name.lower():
                            logger.info(f"âœ… ModÃ¨le Llama dÃ©tectÃ©: {name}")
                            return name
                    
                    # Prendre le premier disponible
                    first_model = models[0]["name"]
                    logger.info(f"âœ… ModÃ¨le dÃ©tectÃ©: {first_model}")
                    return first_model
        except Exception as e:
            logger.warning(f"Impossible de dÃ©tecter le modÃ¨le: {e}")
        
        # Fallback par dÃ©faut
        return "mistral:7b-instruct"
    
    
    def check_health(self) -> bool:
        """VÃ©rifie si Ollama est disponible"""
        try:
            response = self.client.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollama non disponible: {e}")
            return False
    
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse Ã  partir d'un prompt
        
        Args:
            prompt: Question ou instruction utilisateur
            system_prompt: Instructions systÃ¨me optionnelles
            temperature: Override tempÃ©rature par dÃ©faut
            max_tokens: Override max_tokens par dÃ©faut
        
        Returns:
            Texte gÃ©nÃ©rÃ© par le LLM
        """
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature or self.temperature,
                "num_predict": max_tokens or self.max_tokens
            }
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        try:
            logger.info(f"ğŸ¤– GÃ©nÃ©ration avec {self.model}...")
            
            response = self.client.post(
                f"{self.base_url}/api/generate",
                json=payload
            )
            response.raise_for_status()
            
            result = response.json()
            generated_text = result.get("response", "")
            
            logger.info(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e: {len(generated_text)} caractÃ¨res")
            
            return generated_text
        
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration LLM: {e}")
            raise
    
    
    def generate_rag_response(
        self,
        query: str,
        context_chunks: List[Dict],
        max_context_length: int = 3000
    ) -> Dict:
        """
        GÃ©nÃ¨re une rÃ©ponse RAG (Retrieval Augmented Generation)
        
        Args:
            query: Question utilisateur
            context_chunks: Chunks rÃ©cupÃ©rÃ©s de la recherche vectorielle
            max_context_length: Longueur max du contexte (en caractÃ¨res)
        
        Returns:
            Dict avec 'answer', 'sources', 'confidence'
        """
        # 1. Construire le contexte Ã  partir des chunks
        context_parts = []
        sources = []
        total_length = 0
        
        for chunk in context_chunks:
            chunk_text = f"[Document: {chunk['filename']}, Score: {chunk['similarity']:.2f}]\n{chunk['content']}\n"
            
            if total_length + len(chunk_text) > max_context_length:
                break
            
            context_parts.append(chunk_text)
            sources.append({
                "filename": chunk["filename"],
                "chunk_index": chunk["chunk_index"],
                "similarity": chunk["similarity"]
            })
            total_length += len(chunk_text)
        
        context = "\n---\n".join(context_parts)
        
        # 2. Construire le prompt RAG
        system_prompt = """Tu es un assistant IA expert qui rÃ©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.

RÃ¨gles importantes:
- RÃ©ponds UNIQUEMENT avec les informations prÃ©sentes dans le contexte
- Si l'information n'est pas dans le contexte, dis "Je n'ai pas cette information dans les documents fournis"
- Cite toujours tes sources en mentionnant le document utilisÃ©
- Sois prÃ©cis, concis et structurÃ© dans tes rÃ©ponses
- Utilise un ton professionnel mais accessible"""

        user_prompt = f"""Contexte (documents de l'entreprise):
{context}

Question de l'utilisateur:
{query}

RÃ©ponds Ã  la question en te basant sur le contexte ci-dessus. Structure ta rÃ©ponse clairement et cite tes sources."""

        # 3. GÃ©nÃ©rer la rÃ©ponse
        try:
            answer = self.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3  # Faible tempÃ©rature pour rÃ©ponses factuelles
            )
            
            # 4. Estimer la confiance basÃ©e sur les scores de similaritÃ©
            if context_chunks:
                avg_similarity = sum(c["similarity"] for c in context_chunks[:3]) / min(3, len(context_chunks))
                confidence = min(avg_similarity * 100, 95)  # Cap Ã  95%
            else:
                confidence = 0.0
            
            return {
                "answer": answer.strip(),
                "sources": sources,
                "confidence": round(confidence, 1),
                "context_used": len(context_parts)
            }
        
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration RAG: {e}")
            return {
                "answer": f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "context_used": 0
            }
    
    
    def generate_general_response(
        self,
        query: str
    ) -> Dict:
        """
        GÃ©nÃ¨re une rÃ©ponse avec la connaissance gÃ©nÃ©rale du modÃ¨le (sans RAG)
        UtilisÃ© quand aucun document pertinent n'est trouvÃ©
        
        Args:
            query: Question utilisateur
        
        Returns:
            Dict avec 'answer', 'sources', 'confidence'
        """
        system_prompt = """Tu es un assistant IA intelligent et serviable.

RÃ¨gles importantes:
- RÃ©ponds avec ta connaissance gÃ©nÃ©rale
- Si la question nÃ©cessite des informations en temps rÃ©el (mÃ©tÃ©o, actualitÃ©s rÃ©centes, bourse), 
  explique clairement que tu n'as pas accÃ¨s Ã  ces donnÃ©es
- Si tu n'es pas sÃ»r d'une information, dis-le honnÃªtement
- Sois prÃ©cis, concis et structurÃ© dans tes rÃ©ponses
- Utilise un ton professionnel mais accessible"""

        user_prompt = f"""Question:
{query}

RÃ©ponds Ã  cette question avec ta connaissance gÃ©nÃ©rale."""

        try:
            answer = self.generate(
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.7  # TempÃ©rature normale pour rÃ©ponses gÃ©nÃ©rales
            )
            
            return {
                "answer": answer.strip(),
                "sources": [],  # Pas de sources pour la connaissance gÃ©nÃ©rale
                "confidence": 80.0,  # Confiance modÃ©rÃ©e (pas de docs pour vÃ©rifier)
                "context_used": 0
            }
        
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rÃ©ponse gÃ©nÃ©rale: {e}")
            return {
                "answer": f"Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse: {str(e)}",
                "sources": [],
                "confidence": 0.0,
                "context_used": 0
            }


# Instance globale
_llm_instance = None

def get_llm_generator() -> LLMGenerator:
    """Retourne l'instance singleton du LLM generator"""
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = LLMGenerator()
    return _llm_instance


if __name__ == "__main__":
    # Test du LLM
    llm = LLMGenerator()
    
    print(f"\nğŸ¤– LLMGenerator initialisÃ©")
    print(f"ğŸ“Š ModÃ¨le: {llm.model}")
    
    # Test santÃ©
    if llm.check_health():
        print(f"âœ… Ollama disponible")
        
        # Test gÃ©nÃ©ration simple
        response = llm.generate("Dis bonjour en franÃ§ais")
        print(f"\nğŸ’¬ Test gÃ©nÃ©ration:")
        print(f"   {response[:100]}...")
    else:
        print(f"âŒ Ollama non disponible")
        print(f"ğŸ’¡ Installez Ollama: https://ollama.ai")
        print(f"ğŸ’¡ Puis: ollama pull mistral:7b-instruct")
