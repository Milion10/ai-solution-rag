"""
API Routes pour le chat RAG
Endpoint principal pour conversations avec LLM + recherche vectorielle
"""
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import logging
from ai.vector_store import get_vector_store
from ai.llm_factory import get_llm_generator

logger = logging.getLogger(__name__)
router = APIRouter()


class HistoryMessage(BaseModel):
    """Message de l'historique de conversation"""
    role: str = Field(description="R√¥le: 'user' ou 'assistant'")
    content: str = Field(description="Contenu du message")


class ChatRequest(BaseModel):
    """Requ√™te de chat"""
    query: str = Field(None, description="Question de l'utilisateur (deprecated, use 'question')", min_length=1)
    question: str = Field(None, description="Question de l'utilisateur", min_length=1)
    user_id: Optional[str] = Field(None, description="ID de l'utilisateur pour filtrer les documents")
    organization_id: Optional[str] = Field(None, description="ID de l'organisation pour filtrer les documents")
    conversation_id: Optional[str] = Field(None, description="ID de la conversation pour documents priv√©s")
    history: List[HistoryMessage] = Field(default=[], description="Historique des messages pr√©c√©dents")
    top_k: int = Field(default=5, description="Nombre de chunks √† r√©cup√©rer", ge=1, le=20)
    similarity_threshold: float = Field(default=0.3, description="Seuil de similarit√© (0-1)", ge=0.0, le=1.0)
    temperature: Optional[float] = Field(default=None, description="Temp√©rature LLM (0-1)", ge=0.0, le=1.0)
    
    @property
    def user_query(self) -> str:
        """Retourne la question (supporte query et question pour compatibilit√©)"""
        return self.question or self.query


class Source(BaseModel):
    """Source cit√©e dans la r√©ponse"""
    filename: str
    chunk_index: int
    similarity: float


class ChatResponse(BaseModel):
    """R√©ponse du chat"""
    query: str
    answer: str
    sources: List[Source]
    confidence: float
    context_used: int
    chunks_found: int


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    Endpoint principal du chat RAG
    
    Process:
    1. Recherche chunks similaires dans pgvector
    2. Construit contexte √† partir des chunks
    3. G√©n√®re r√©ponse avec LLM + contexte
    4. Retourne r√©ponse + sources cit√©es
    
    Args:
        request: ChatRequest avec query, top_k, similarity_threshold
    
    Returns:
        ChatResponse avec answer, sources, confidence
    """
    logger.info(f"üí¨ Chat request: '{request.user_query[:50]}...'")
    logger.info(f"   üîë user_id: {request.user_id}, org_id: {request.organization_id}, conv_id: {request.conversation_id}")
    
    try:
        # 1. Recherche vectorielle (avec filtrage par user_id et organization_id si fourni)
        vector_store = get_vector_store()
        logger.info(f"   üîç Recherche avec conversation_id={request.conversation_id}")
        chunks = vector_store.search_similar(
            query_text=request.user_query,
            top_k=request.top_k,
            similarity_threshold=0.0,  # R√©cup√©rer tous les chunks pour filtrer ensuite
            user_id=request.user_id,
            organization_id=request.organization_id,
            conversation_id=request.conversation_id
        )
        logger.info(f"   üì¶ {len(chunks)} chunks trouv√©s")
        
        # Filtrer les chunks par seuil de similarit√© pour d√©terminer la pertinence
        # Seuil √† 0.4 (40%) : en dessous, la similarit√© est trop faible pour √™tre pertinente
        RELEVANCE_THRESHOLD = 0.4
        relevant_chunks = [chunk for chunk in chunks if chunk['similarity'] >= RELEVANCE_THRESHOLD]
        
        # 2. V√©rifier la disponibilit√© du LLM
        llm = get_llm_generator()
        if not llm.check_health():
            raise HTTPException(
                status_code=503,
                detail="Le service LLM (Ollama) n'est pas disponible. Veuillez v√©rifier qu'Ollama est install√© et d√©marr√©."
            )
        
        # 3. D√©cider du mode : RAG (documents pertinents) ou G√©n√©ral (connaissance du mod√®le)
        if relevant_chunks:
            # MODE RAG : Documents pertinents trouv√©s
            logger.info(f"  üìö Mode RAG - {len(relevant_chunks)} chunks pertinents (score > {RELEVANCE_THRESHOLD})")
            logger.info(f"  üí¨ Historique: {len(request.history)} messages")
            
            rag_result = llm.generate_rag_response(
                query=request.user_query,
                context_chunks=relevant_chunks,
                max_context_length=3000,
                conversation_history=request.history
            )
            
        else:
            # MODE G√âN√âRAL : Pas de documents pertinents, utiliser la connaissance du mod√®le
            logger.info(f"  üß† Mode G√©n√©ral - Aucun document pertinent (seuil: {RELEVANCE_THRESHOLD})")
            logger.info(f"  üí¨ Historique: {len(request.history)} messages")
            
            # G√©n√©rer une r√©ponse avec la connaissance g√©n√©rale du mod√®le
            rag_result = llm.generate_general_response(
                query=request.user_query,
                conversation_history=request.history
            )
        
        logger.info(f"  ‚úÖ R√©ponse g√©n√©r√©e (confidence: {rag_result.get('confidence', 100)}%)")
        
        # 4. Formater la r√©ponse
        return ChatResponse(
            query=request.user_query,
            answer=rag_result["answer"],
            sources=[Source(**s) for s in rag_result.get("sources", [])],
            confidence=rag_result.get("confidence", 100),
            context_used=rag_result.get("context_used", 0),
            chunks_found=len(relevant_chunks) if relevant_chunks else 0
        )
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur chat: {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors du traitement de la requ√™te: {str(e)}")


@router.get("/chat/health")
async def chat_health():
    """
    V√©rifie la sant√© du service de chat
    
    Returns:
        Status du LLM (Ollama) et du vector store
    """
    try:
        llm = get_llm_generator()
        ollama_available = llm.check_health()
        
        vector_store = get_vector_store()
        
        return {
            "status": "healthy" if ollama_available else "degraded",
            "ollama_available": ollama_available,
            "ollama_model": llm.model,
            "vector_store_initialized": True,
            "embedding_dim": vector_store.embedding_dim,
            "message": "Service chat op√©rationnel" if ollama_available else "Ollama non disponible - installez et d√©marrez Ollama"
        }
    
    except Exception as e:
        logger.error(f"Erreur health check: {e}")
        return {
            "status": "unhealthy",
            "ollama_available": False,
            "error": str(e)
        }


@router.get("/chat/test")
async def chat_test(q: str = "Quelle est la vision du produit ?"):
    """
    Endpoint de test rapide du chat
    
    Args:
        q: Question de test (query parameter)
    
    Returns:
        R√©ponse du chat avec sources
    """
    request = ChatRequest(query=q, top_k=3, similarity_threshold=0.3)
    return await chat(request)
